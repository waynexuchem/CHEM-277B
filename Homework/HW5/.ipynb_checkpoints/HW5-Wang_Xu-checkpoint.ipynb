{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b999ec2b",
   "metadata": {},
   "source": [
    "# Chem277B: Machine Learning Algorithms\n",
    "\n",
    "## Homework assignment #5: Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61df6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776de74e",
   "metadata": {},
   "source": [
    "### 1. Bayeâ€™s Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1295c",
   "metadata": {},
   "source": [
    "(a) From the given data, the categories of testing results and their probabilities within proportion have been summarized in the table below:\n",
    "\n",
    "Category | Probability within Proportion | Kidney Disease Positive | Marker | Proportion\n",
    "--- | --- | --- | --- | ---\n",
    "P[+\\|M] | 0.95 | $+$ | $+$ | 0.95 * 0.01\n",
    "P[-\\|M] | (1-0.95) | $-$ | $+$ | (1-0.95) * 0.01\n",
    "P[+\\|not M] | (1-0.95) | $+$ | $-$ | (1-0.95) * 0.99\n",
    "P[-\\|not M] | 0.95 | $-$ | $-$ | 0.95 * 0.99\n",
    "\n",
    "Hence the quantities for the questions are:\n",
    "\n",
    "(a1) P[-\\|M] = (1-0.95) = 0.05 within its proportion, the absolute probability is 0.05 * 0.01 = 0.0005\n",
    "\n",
    "(a2) P[+\\|not M] = (1-0.95) = 0.05 within its proportion, the absolute probability is 0.05 * 0.99 = 0.0495\n",
    "\n",
    "(a3) P[not M] = (1-0.95) * 0.99 + 0.95 * 0.99 = 0.99, or 1 - 0.01 = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac771d",
   "metadata": {},
   "source": [
    "(b) Using the Baye's Theorem, we try to differentiate between positive marker + positive test and positive marker + negative test. Hence the calculation is defined as:\n",
    "$$ P[M|+] = \\frac{P[+|M] * P[M]}{P[+|M] * P[M] + P[+|not M] * P[not M]} $$   \n",
    "$$ = \\frac{0.95 \\times 0.01}{(0.95 \\times 0.01) + (0.05 \\times 0.99)} = 0.161 $$\n",
    "\n",
    "Hence the chance of testing positive and actually have the marker is 16.1%. It warrants additional testing to confirm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24c12fa",
   "metadata": {},
   "source": [
    "(c) When P[M] = 0.10, the categories and probabilities become the following:\n",
    "\n",
    "Category | Probability within Proportion | Kidney Disease Positive | Marker | Proportion\n",
    "--- | --- | --- | --- | ---\n",
    "P[+\\|M] | 0.95 | $+$ | $+$ | 0.95 * 0.10\n",
    "P[-\\|M] | (1-0.95) | $-$ | $+$ | (1-0.95) * 0.10\n",
    "P[+\\|not M] | (1-0.95) | $+$ | $-$ | (1-0.95) * 0.90\n",
    "P[-\\|not M] | 0.95 | $-$ | $-$ | 0.95 * 0.90\n",
    "\n",
    "Hence with the new frequency, the individual who test positive actually has the marker is:\n",
    "$$ P[M|+]' = \\frac{P[+|M]' * P[M]'}{P[+|M]' * P[M]' + P[+|not M]' * P[not M]'} $$   \n",
    "$$ = \\frac{0.95 \\times 0.1}{(0.95 \\times 0.1) + (0.05 \\times 0.9)} = 0.679 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e021ba",
   "metadata": {},
   "source": [
    "### 2. Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc077ea1",
   "metadata": {},
   "source": [
    "(a) The finished codes are shown below.\n",
    "\n",
    "I chose Gaussian distribution because it's a widely used normal probability distributions in statistics, data analysis and visualization. \n",
    "\n",
    "The Gaussian distribution has a bell curve with mean and standard deviation, hence suitable for modeling many real-world phenomena.\n",
    "\n",
    "With the finished function, I calculated that a wine from cultivar 1 has a 23.24% probability of containinh 13% Alcohol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e628173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self):\n",
    "        self.type_indices={}    # store the indices of wines that belong to each cultivar as a boolean array of length 178\n",
    "        self.type_stats={}      # store the mean and std of each cultivar\n",
    "        self.ndata = 0\n",
    "        self.trained=False\n",
    "    \n",
    "    @staticmethod\n",
    "    def gaussian(x,mean,std):\n",
    "        exponent = -(x - mean)**2 / (2 * std**2)\n",
    "        \n",
    "        return (np.exp(exponent) / (np.sqrt(2 * np.pi) * std))\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_statistics(x_values):\n",
    "        # Returns a list with length of input features. Each element is a tuple, with the input feature's average and standard deviation\n",
    "        n_feats=x_values.shape[1]\n",
    "        return [(np.average(x_values[:,n]),np.std(x_values[:,n])) for n in range(n_feats)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_prob(x_input,stats):\n",
    "        \"\"\"Calculate the probability that the input features belong to a specific class(P(X|C)), defined by the statistics of features in that class\n",
    "        x_input: np.array shape(nfeatures)\n",
    "        stats: list of tuple [(mean1,std1),(means2,std2),...]\n",
    "        \"\"\" \n",
    "        init_prob = 1 \n",
    "        for i in range(len(x_input)):\n",
    "            mean, std = stats[i]\n",
    "            init_prob *= NaiveBayesClassifier.gaussian(x_input[i], mean, std)\n",
    "        return init_prob\n",
    "    \n",
    "    def fit(self,xs,ys):\n",
    "        # Train the classifier by calculating the statistics of different features in each class\n",
    "        self.ndata = len(ys)\n",
    "        for y in set(ys):\n",
    "            type_filter= (ys==y)\n",
    "            self.type_indices[y]=type_filter\n",
    "            self.type_stats[y]=self.calculate_statistics(xs[type_filter])\n",
    "        self.trained=True\n",
    "            \n",
    "    def predict(self,xs):\n",
    "        # Do the prediction by outputing the class that has highest probability\n",
    "        if len(xs.shape)>1:\n",
    "            print(\"Only accepts one sample at a time!\")\n",
    "        if self.trained:\n",
    "            guess=None\n",
    "            max_prob=0\n",
    "            # P(C|X) = P(X|C)*P(C) / sum_i(P(X|C_i)*P(C_i)) (deniminator for normalization only, can be ignored)\n",
    "            for y_type in self.type_stats:\n",
    "                pre = sum(self.type_indices[y_type]) / self.ndata\n",
    "                prob= self.calculate_prob(xs, self.type_stats[y_type]) * pre\n",
    "                if prob>max_prob:\n",
    "                    max_prob=prob\n",
    "                    guess=y_type\n",
    "            return guess\n",
    "        else:\n",
    "            print(\"Please train the classifier first!\")\n",
    "            \n",
    "def calculate_accuracy(model,xs,ys):\n",
    "    y_pred=np.zeros_like(ys)\n",
    "    for idx,x in enumerate(xs):\n",
    "        y_pred[idx]=model.predict(x)\n",
    "    return np.sum(ys==y_pred)/len(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444af9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol %</th>\n",
       "      <th>Malic Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alkalinity</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Phenols.1</th>\n",
       "      <th>Proantho-cyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280 315</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Start assignment</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.12</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.32</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1280</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol %  Malic Acid   Ash  Alkalinity   Mg  Phenols  Flavanoids  \\\n",
       "0      14.23        1.71  2.43        15.6  127      2.8        3.06   \n",
       "1      13.24        2.59  2.87        21.0  118      2.8        2.69   \n",
       "2      14.83        1.64  2.17        14.0   97      2.8        2.98   \n",
       "3      14.12        1.48  2.32        16.8   95      2.2        2.43   \n",
       "4      13.75        1.73  2.41        16.0   89      2.6        2.76   \n",
       "\n",
       "   Phenols.1  Proantho-cyanins  Color intensity   Hue  OD280 315  Proline  \\\n",
       "0       0.28              2.29             5.64  1.04       3.92     1065   \n",
       "1       0.39              1.82             4.32  1.04       2.93      735   \n",
       "2       0.29              1.98             5.20  1.08       2.85     1045   \n",
       "3       0.26              1.57             5.00  1.17       2.82     1280   \n",
       "4       0.29              1.81             5.60  1.15       2.90     1320   \n",
       "\n",
       "   Start assignment  ranking  \n",
       "0                 1        1  \n",
       "1                 1        1  \n",
       "2                 1        1  \n",
       "3                 1        1  \n",
       "4                 1        1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import wines.csv\n",
    "wines = pd.read_csv('wines.csv')\n",
    "wines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f099a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wine from cultivar 1 has a 23.24% probability of containinh 13% Alcohol\n"
     ]
    }
   ],
   "source": [
    "# Define the instance from the Naive Bayes Classifier\n",
    "nbc = NaiveBayesClassifier()\n",
    "\n",
    "# Fit the Naive Bayes Classifier\n",
    "nbc.fit(wines.loc[:, 'Alcohol %':'Proline'].values, wines.loc[:, 'ranking'].values)\n",
    "\n",
    "# First get the stats for cultivar 1\n",
    "type_stats = nbc.type_stats[1]\n",
    "\n",
    "# Then calculate the probability of alcohol% = 13\n",
    "probability = nbc.gaussian(13, type_stats[0][0], type_stats[0][1])\n",
    "\n",
    "print(f\"A wine from cultivar 1 has a {round(probability*100, 2)}% probability of containinh 13% Alcohol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd7883",
   "metadata": {},
   "source": [
    "(b) After 3-fold training, I can achieve close to 100% accuracy in very short term. The Naive Baye's method performs much better and much faster than the simulated annealing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9685d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol %</th>\n",
       "      <th>Malic Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alkalinity</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Phenols.1</th>\n",
       "      <th>Proantho-cyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280 315</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Start assignment</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.259772</td>\n",
       "      <td>-0.625086</td>\n",
       "      <td>-0.718336</td>\n",
       "      <td>-1.650049</td>\n",
       "      <td>-0.192495</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.954502</td>\n",
       "      <td>-0.578985</td>\n",
       "      <td>0.681738</td>\n",
       "      <td>0.061386</td>\n",
       "      <td>0.537671</td>\n",
       "      <td>0.336606</td>\n",
       "      <td>0.949319</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.382733</td>\n",
       "      <td>-0.768712</td>\n",
       "      <td>-0.170035</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>-0.332922</td>\n",
       "      <td>-0.152402</td>\n",
       "      <td>0.402320</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.036617</td>\n",
       "      <td>-0.025128</td>\n",
       "      <td>0.932531</td>\n",
       "      <td>0.294232</td>\n",
       "      <td>1.697675</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.925685</td>\n",
       "      <td>-0.544297</td>\n",
       "      <td>0.158946</td>\n",
       "      <td>-1.049479</td>\n",
       "      <td>-0.754202</td>\n",
       "      <td>0.488531</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.578985</td>\n",
       "      <td>0.383884</td>\n",
       "      <td>0.234414</td>\n",
       "      <td>0.844785</td>\n",
       "      <td>0.407228</td>\n",
       "      <td>1.825055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol %  Malic Acid       Ash  Alkalinity        Mg   Phenols  \\\n",
       "0   1.518613   -0.562250  0.232053   -1.169593  1.913905  0.808997   \n",
       "1   0.295700    0.227694  1.840403    0.451946  1.281985  0.808997   \n",
       "2   2.259772   -0.625086 -0.718336   -1.650049 -0.192495  0.808997   \n",
       "3   1.382733   -0.768712 -0.170035   -0.809251 -0.332922 -0.152402   \n",
       "4   0.925685   -0.544297  0.158946   -1.049479 -0.754202  0.488531   \n",
       "\n",
       "   Flavanoids  Phenols.1  Proantho-cyanins  Color intensity       Hue  \\\n",
       "0    1.034819  -0.659563          1.224884         0.251717  0.362177   \n",
       "1    0.663351   0.226796          0.401404        -0.319276  0.362177   \n",
       "2    0.954502  -0.578985          0.681738         0.061386  0.537671   \n",
       "3    0.402320  -0.820719         -0.036617        -0.025128  0.932531   \n",
       "4    0.733629  -0.578985          0.383884         0.234414  0.844785   \n",
       "\n",
       "   OD280 315   Proline  Start assignment  ranking  \n",
       "0   1.847920  1.013009                 1        1  \n",
       "1   0.449601 -0.037874                 1        1  \n",
       "2   0.336606  0.949319                 1        1  \n",
       "3   0.294232  1.697675                 1        1  \n",
       "4   0.407228  1.825055                 1        1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First normalize the wines dataframe\n",
    "wines_norm = wines.loc[:, 'Alcohol %':'Proline']\n",
    "wines_norm = (wines_norm - np.mean(wines_norm, axis=0)) / np.std(wines_norm, axis=0)\n",
    "wines_norm = wines_norm.merge(wines[['Start assignment','ranking']], left_index=True, right_index=True)\n",
    "wines_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aa1a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "Accuracy: 0.9661016949152542\n",
      "Accuracy: 1.0\n",
      "Average accuracy after 3-fold training is 0.9720338983050847\n"
     ]
    }
   ],
   "source": [
    "# Divide the normalized wines data into 3-fold training and testing groups\n",
    "# and use 2/3 training and 1/3 testing for the three divisions\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "xs = wines.loc[:, 'Alcohol %':'Proline'].values \n",
    "ys = wines.loc[:, 'ranking'].values\n",
    "nbc = NaiveBayesClassifier()\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in kf.split(xs):\n",
    "    x_train, x_test = xs[train_index], xs[test_index]\n",
    "    y_train, y_test = ys[train_index], ys[test_index]\n",
    "\n",
    "    # train the classifier\n",
    "    nbc.fit(x_train,y_train)\n",
    "    accuracy.append(calculate_accuracy(nbc,x_test,y_test))\n",
    "    print(f'Accuracy: {calculate_accuracy(nbc,x_test,y_test)}')\n",
    "print(f'Average accuracy after 3-fold training is {np.array(accuracy).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644ce5a",
   "metadata": {},
   "source": [
    "### 3. Softmax and Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373042e2",
   "metadata": {},
   "source": [
    "(a) I did one PyTorch model without softmax and one PyTorch model with softmax. The output without softmax is a cluster of large positive or negative values. The output with softmax is more like probabilities that sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4e361ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -95.7595,  -80.1433, -271.1181],\n",
      "        [ -70.3680,  -55.5473, -186.5312],\n",
      "        [ -89.5526,  -77.3873, -266.7516],\n",
      "        [-105.8480,  -94.7881, -328.4174],\n",
      "        [-107.6909,  -97.2809, -338.8018],\n",
      "        [-109.6877,  -95.2311, -327.6151],\n",
      "        [ -75.2029,  -59.1946, -197.1850],\n",
      "        [ -86.9012,  -75.8067, -259.8968],\n",
      "        [ -74.8449,  -63.2743, -215.8418],\n",
      "        [-105.9978,  -95.4717, -329.8702],\n",
      "        [ -88.4649,  -76.9907, -264.4617],\n",
      "        [-108.5639,  -93.0019, -315.7316],\n",
      "        [ -79.9209,  -66.1011, -224.1377],\n",
      "        [ -93.8444,  -82.1973, -283.1660],\n",
      "        [ -93.6766,  -81.3249, -279.8405],\n",
      "        [ -79.4933,  -66.3302, -225.2316],\n",
      "        [ -91.1583,  -78.3610, -270.7823],\n",
      "        [-104.6139,  -93.3383, -324.4534],\n",
      "        [-102.0187,  -88.3862, -303.7653],\n",
      "        [-108.2847,  -95.1919, -328.8627],\n",
      "        [ -64.1282,  -51.7691, -172.8694],\n",
      "        [ -43.1073,  -31.5106, -105.4305],\n",
      "        [ -47.6285,  -37.0139, -126.4444],\n",
      "        [ -77.7044,  -65.4711, -222.4468],\n",
      "        [ -52.6122,  -38.7805, -125.9749],\n",
      "        [ -56.5051,  -47.1741, -161.2987],\n",
      "        [ -51.6823,  -39.4758, -131.3857],\n",
      "        [ -47.4287,  -34.8122, -113.2047],\n",
      "        [ -60.9081,  -50.8997, -173.8416],\n",
      "        [ -54.7720,  -46.7230, -160.1270],\n",
      "        [ -92.5969,  -72.4996, -237.2635],\n",
      "        [ -65.2154,  -48.7314, -158.0107],\n",
      "        [ -42.2817,  -30.2529, -101.8373],\n",
      "        [ -65.3198,  -53.5460, -180.6982],\n",
      "        [ -42.4807,  -31.7434, -104.5706],\n",
      "        [ -56.0758,  -42.3140, -141.3992],\n",
      "        [ -44.3542,  -33.0702, -109.6971],\n",
      "        [ -39.9226,  -29.4330,  -97.2258],\n",
      "        [ -41.9280,  -30.9954, -102.9244],\n",
      "        [ -58.9969,  -47.1536, -158.7496],\n",
      "        [ -39.0347,  -27.9414,  -95.3846],\n",
      "        [ -37.5140,  -26.1886,  -85.9839],\n",
      "        [ -52.5705,  -42.9040, -147.9189],\n",
      "        [ -53.9974,  -41.8098, -142.2261],\n",
      "        [ -58.1301,  -44.9303, -152.1356],\n",
      "        [ -53.6420,  -42.8173, -147.0570],\n",
      "        [ -56.4847,  -44.4313, -149.7079],\n",
      "        [ -50.9161,  -39.1205, -131.8629],\n",
      "        [ -40.2738,  -30.0874, -104.7068],\n",
      "        [ -55.1265,  -40.5552, -138.4230],\n",
      "        [ -47.2168,  -32.2037, -106.1813],\n",
      "        [ -46.2232,  -34.4733, -120.5233],\n",
      "        [ -64.0021,  -50.8870, -172.5027],\n",
      "        [ -64.9461,  -50.8716, -175.9956],\n",
      "        [ -60.6507,  -49.6607, -174.0285],\n",
      "        [ -59.6397,  -48.0405, -167.2719],\n",
      "        [ -65.4994,  -53.9129, -188.2240],\n",
      "        [ -67.7058,  -55.2292, -190.8036],\n",
      "        [ -90.5299,  -78.4755, -268.1708],\n",
      "        [-122.7202, -109.5233, -379.0020],\n",
      "        [-120.7422, -107.4394, -371.4403],\n",
      "        [ -89.3664,  -77.3397, -266.8197],\n",
      "        [ -96.4089,  -84.4749, -293.8468],\n",
      "        [ -98.1854,  -84.0856, -288.8749],\n",
      "        [ -69.9950,  -57.3209, -195.9571],\n",
      "        [ -78.1801,  -63.0056, -211.5188],\n",
      "        [ -81.6687,  -68.4600, -233.4466],\n",
      "        [-106.9510,  -94.9313, -329.7242],\n",
      "        [ -86.7286,  -73.9335, -252.8878],\n",
      "        [ -95.0400,  -81.9118, -280.2427],\n",
      "        [ -87.9826,  -76.1874, -260.9209],\n",
      "        [ -73.8215,  -57.3670, -191.7273],\n",
      "        [ -63.7275,  -50.7680, -172.3642],\n",
      "        [ -93.3901,  -80.3729, -275.9935],\n",
      "        [ -85.6678,  -73.0847, -251.1188],\n",
      "        [ -96.1325,  -84.3870, -293.8909],\n",
      "        [-115.7293, -102.2474, -352.2450],\n",
      "        [ -93.7798,  -79.4673, -270.3569],\n",
      "        [ -50.8155,  -39.9812, -131.2031],\n",
      "        [ -59.1775,  -47.5665, -160.0092],\n",
      "        [ -41.3422,  -27.7701,  -88.7323],\n",
      "        [ -47.7472,  -38.1787, -129.4783],\n",
      "        [ -41.9844,  -30.6733, -103.5136],\n",
      "        [ -46.3154,  -35.5393, -119.7110],\n",
      "        [ -41.0043,  -29.5392,  -98.1409],\n",
      "        [ -75.1354,  -57.8211, -189.2939],\n",
      "        [ -32.9722,  -21.7195,  -69.0966],\n",
      "        [ -63.8454,  -53.1075, -182.0839],\n",
      "        [ -52.9921,  -42.5161, -143.5636],\n",
      "        [ -46.4499,  -36.0199, -121.7188],\n",
      "        [ -47.2032,  -36.9527, -125.7045],\n",
      "        [ -33.8721,  -22.2086,  -71.9166],\n",
      "        [ -53.4146,  -42.6067, -142.7589],\n",
      "        [ -45.8384,  -33.6798, -110.2899],\n",
      "        [ -60.5284,  -50.5022, -171.3946],\n",
      "        [ -36.2448,  -24.2678,  -77.3887],\n",
      "        [ -63.0564,  -51.1417, -173.1173],\n",
      "        [ -58.8220,  -45.8668, -154.3202],\n",
      "        [ -48.1790,  -37.8065, -125.5902],\n",
      "        [ -38.0483,  -27.7569,  -93.3553],\n",
      "        [ -53.0801,  -42.4808, -143.2783],\n",
      "        [ -37.9239,  -26.5384,  -88.1518],\n",
      "        [ -46.6358,  -35.1526, -118.4888],\n",
      "        [ -52.9951,  -39.7576, -133.9356],\n",
      "        [ -63.7739,  -53.1868, -183.9596],\n",
      "        [ -56.6567,  -45.0805, -152.4304],\n",
      "        [ -68.2721,  -57.9437, -199.1737],\n",
      "        [ -52.0392,  -40.6550, -139.2529],\n",
      "        [ -73.9494,  -61.9691, -211.7534],\n",
      "        [ -58.9663,  -47.5144, -164.8894],\n",
      "        [ -76.0654,  -64.9467, -225.1627],\n",
      "        [ -49.0072,  -37.9552, -131.3719],\n",
      "        [ -55.8980,  -42.9190, -144.2426],\n",
      "        [ -63.5942,  -50.5809, -171.2624],\n",
      "        [ -48.9179,  -37.9178, -131.6362],\n",
      "        [ -50.1381,  -38.2544, -128.8955],\n",
      "        [ -45.1788,  -34.1721, -118.3403],\n",
      "        [ -99.8508,  -87.4786, -303.4079],\n",
      "        [-106.7419,  -95.4956, -330.5671],\n",
      "        [-110.9604,  -97.0445, -331.5586],\n",
      "        [-123.7389, -111.5977, -387.5805],\n",
      "        [-126.2076, -113.7205, -396.4001],\n",
      "        [-110.5025,  -97.1671, -335.5422],\n",
      "        [-136.3220, -123.7616, -431.3432],\n",
      "        [ -77.9980,  -63.3852, -214.6196],\n",
      "        [ -89.3988,  -77.4158, -264.6501],\n",
      "        [ -99.4892,  -88.5956, -306.3633],\n",
      "        [-124.1193, -112.0037, -388.9803],\n",
      "        [ -80.6667,  -68.5250, -234.9812],\n",
      "        [ -74.7416,  -59.7568, -201.2769],\n",
      "        [ -86.8939,  -76.3856, -265.1540],\n",
      "        [ -91.2729,  -78.7977, -271.8964],\n",
      "        [-106.1120,  -92.9291, -322.3038],\n",
      "        [ -97.4838,  -83.4479, -286.0666],\n",
      "        [ -87.3043,  -72.6440, -246.6375],\n",
      "        [-106.1832,  -94.0076, -325.4984],\n",
      "        [ -47.0953,  -34.2192, -112.8618],\n",
      "        [ -63.4865,  -50.6691, -172.2142],\n",
      "        [ -70.5792,  -57.3044, -190.6053],\n",
      "        [ -75.0449,  -56.4108, -180.7851],\n",
      "        [ -91.4002,  -75.2161, -251.5265],\n",
      "        [ -78.5728,  -66.8261, -226.2947],\n",
      "        [ -45.2534,  -32.7163, -107.5243],\n",
      "        [ -47.8508,  -35.0571, -116.7137],\n",
      "        [ -48.6012,  -37.7607, -130.6048],\n",
      "        [ -48.8070,  -37.4591, -125.7198],\n",
      "        [ -44.7871,  -33.9485, -113.9771],\n",
      "        [ -39.4460,  -26.7756,  -85.8938],\n",
      "        [ -43.6103,  -32.3360, -107.6418],\n",
      "        [ -60.5434,  -49.1194, -167.6660],\n",
      "        [ -35.6747,  -24.3274,  -78.7909],\n",
      "        [ -47.1800,  -36.5285, -123.9038],\n",
      "        [ -36.2320,  -24.9942,  -81.3378],\n",
      "        [ -41.0823,  -27.3728,  -86.0754],\n",
      "        [ -51.0255,  -35.5192, -117.1997],\n",
      "        [ -40.7060,  -27.9433,  -91.7698],\n",
      "        [ -38.8315,  -27.9552,  -95.4119],\n",
      "        [ -39.5275,  -28.7804,  -95.1505],\n",
      "        [ -63.7631,  -48.4041, -159.2106],\n",
      "        [ -58.8792,  -48.3266, -165.1284],\n",
      "        [ -63.0256,  -51.4977, -176.6051],\n",
      "        [ -49.7291,  -37.9999, -130.5413],\n",
      "        [ -76.9455,  -63.4678, -217.5637],\n",
      "        [ -55.9494,  -45.4670, -158.8678],\n",
      "        [ -53.3975,  -37.4375, -125.4094],\n",
      "        [ -50.0229,  -35.3705, -120.1836],\n",
      "        [ -61.7129,  -49.0515, -170.6138],\n",
      "        [ -60.6729,  -47.8728, -161.9878],\n",
      "        [ -64.1084,  -52.6943, -184.3833],\n",
      "        [ -60.4054,  -47.5032, -166.5674],\n",
      "        [ -56.3295,  -44.6045, -156.6009],\n",
      "        [ -56.0662,  -44.7320, -155.9557],\n",
      "        [ -68.2930,  -55.3038, -190.7732],\n",
      "        [ -60.3732,  -46.4871, -159.5021],\n",
      "        [ -76.4289,  -61.6658, -211.7994],\n",
      "        [ -77.3098,  -62.5640, -213.1622],\n",
      "        [ -52.7916,  -40.5824, -141.4767],\n",
      "        [ -48.2669,  -38.1544, -129.4740]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# First convert the features and labels to PyTorch tensors\n",
    "pytorch_features = torch.tensor(wines.loc[:, 'Alcohol %':'Proline'].values , dtype=torch.float32)\n",
    "pytorch_labels = torch.tensor(wines.loc[:, 'ranking'].values, dtype=torch.int64)\n",
    "\n",
    "# Then define a pytorch model without softmax\n",
    "model_no_softmax = nn.Sequential(\n",
    "    nn.Linear(pytorch_features.shape[1], len(np.unique(pytorch_labels)))\n",
    ")\n",
    "\n",
    "# Then pass the data through the model once without backpropagation\n",
    "outputs_no_softmax = model_no_softmax(pytorch_features)\n",
    "\n",
    "# Finally print out the outputs_no_softmax\n",
    "print(outputs_no_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26502232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8472e-04, 0.0000e+00, 9.9982e-01],\n",
      "        [8.6887e-05, 0.0000e+00, 9.9991e-01],\n",
      "        [3.2321e-02, 0.0000e+00, 9.6768e-01],\n",
      "        [5.2833e-01, 0.0000e+00, 4.7167e-01],\n",
      "        [8.3785e-01, 0.0000e+00, 1.6215e-01],\n",
      "        [1.6370e-02, 0.0000e+00, 9.8363e-01],\n",
      "        [8.5346e-06, 0.0000e+00, 9.9999e-01],\n",
      "        [7.0744e-02, 0.0000e+00, 9.2926e-01],\n",
      "        [1.5047e-02, 0.0000e+00, 9.8495e-01],\n",
      "        [6.7305e-01, 0.0000e+00, 3.2695e-01],\n",
      "        [5.4550e-02, 0.0000e+00, 9.4545e-01],\n",
      "        [8.9474e-04, 0.0000e+00, 9.9911e-01],\n",
      "        [4.5460e-04, 0.0000e+00, 9.9955e-01],\n",
      "        [1.0055e-01, 0.0000e+00, 8.9945e-01],\n",
      "        [3.4522e-02, 0.0000e+00, 9.6548e-01],\n",
      "        [1.5719e-03, 0.0000e+00, 9.9843e-01],\n",
      "        [2.9096e-02, 0.0000e+00, 9.7090e-01],\n",
      "        [5.5085e-01, 0.0000e+00, 4.4915e-01],\n",
      "        [1.0845e-02, 0.0000e+00, 9.8915e-01],\n",
      "        [7.0314e-02, 0.0000e+00, 9.2969e-01],\n",
      "        [2.5039e-04, 0.0000e+00, 9.9975e-01],\n",
      "        [3.3493e-04, 8.6922e-32, 9.9967e-01],\n",
      "        [2.2942e-03, 4.1237e-35, 9.9771e-01],\n",
      "        [4.4389e-03, 0.0000e+00, 9.9556e-01],\n",
      "        [1.5888e-05, 8.3079e-39, 9.9998e-01],\n",
      "        [5.6468e-02, 6.0180e-41, 9.4353e-01],\n",
      "        [1.9038e-04, 8.1258e-38, 9.9981e-01],\n",
      "        [4.3109e-05, 1.4603e-34, 9.9996e-01],\n",
      "        [3.2355e-02, 1.6816e-44, 9.6764e-01],\n",
      "        [2.4256e-01, 1.1164e-39, 7.5744e-01],\n",
      "        [1.2097e-07, 0.0000e+00, 1.0000e+00],\n",
      "        [9.9605e-07, 0.0000e+00, 1.0000e+00],\n",
      "        [7.7131e-04, 8.2828e-31, 9.9923e-01],\n",
      "        [2.4262e-03, 0.0000e+00, 9.9757e-01],\n",
      "        [7.4628e-04, 5.7368e-31, 9.9925e-01],\n",
      "        [2.1208e-04, 1.6608e-41, 9.9979e-01],\n",
      "        [3.6215e-04, 1.9108e-32, 9.9964e-01],\n",
      "        [9.2764e-04, 4.0758e-29, 9.9907e-01],\n",
      "        [1.1742e-03, 1.7986e-30, 9.9883e-01],\n",
      "        [1.6996e-03, 2.2841e-43, 9.9830e-01],\n",
      "        [5.7814e-03, 8.8424e-29, 9.9422e-01],\n",
      "        [4.8042e-04, 4.2283e-27, 9.9952e-01],\n",
      "        [7.3373e-02, 1.3141e-38, 9.2663e-01],\n",
      "        [5.1348e-04, 3.1990e-40, 9.9949e-01],\n",
      "        [2.4543e-04, 2.6204e-43, 9.9975e-01],\n",
      "        [4.3102e-03, 3.9934e-40, 9.9569e-01],\n",
      "        [6.9607e-04, 4.0834e-42, 9.9930e-01],\n",
      "        [6.6453e-04, 9.5720e-38, 9.9934e-01],\n",
      "        [6.4963e-03, 3.2815e-30, 9.9350e-01],\n",
      "        [2.3431e-05, 2.5714e-42, 9.9998e-01],\n",
      "        [2.6486e-06, 8.5243e-36, 1.0000e+00],\n",
      "        [8.1758e-04, 1.2263e-35, 9.9918e-01],\n",
      "        [2.9942e-04, 0.0000e+00, 9.9970e-01],\n",
      "        [1.7692e-04, 0.0000e+00, 9.9982e-01],\n",
      "        [6.4611e-03, 0.0000e+00, 9.9354e-01],\n",
      "        [1.8833e-03, 2.8026e-45, 9.9812e-01],\n",
      "        [1.7553e-02, 0.0000e+00, 9.8245e-01],\n",
      "        [3.4296e-03, 0.0000e+00, 9.9657e-01],\n",
      "        [1.5172e-02, 0.0000e+00, 9.8483e-01],\n",
      "        [2.6207e-01, 0.0000e+00, 7.3793e-01],\n",
      "        [1.8598e-01, 0.0000e+00, 8.1402e-01],\n",
      "        [2.6553e-02, 0.0000e+00, 9.7345e-01],\n",
      "        [1.8363e-01, 0.0000e+00, 8.1637e-01],\n",
      "        [6.2811e-03, 0.0000e+00, 9.9372e-01],\n",
      "        [3.5683e-03, 0.0000e+00, 9.9643e-01],\n",
      "        [1.3672e-04, 0.0000e+00, 9.9986e-01],\n",
      "        [3.2982e-03, 0.0000e+00, 9.9670e-01],\n",
      "        [4.8374e-01, 0.0000e+00, 5.1626e-01],\n",
      "        [1.0531e-02, 0.0000e+00, 9.8947e-01],\n",
      "        [1.3018e-02, 0.0000e+00, 9.8698e-01],\n",
      "        [3.0631e-02, 0.0000e+00, 9.6937e-01],\n",
      "        [8.9407e-06, 0.0000e+00, 9.9999e-01],\n",
      "        [1.0294e-03, 0.0000e+00, 9.9897e-01],\n",
      "        [2.8042e-02, 0.0000e+00, 9.7196e-01],\n",
      "        [1.0847e-02, 0.0000e+00, 9.8915e-01],\n",
      "        [1.5644e-01, 0.0000e+00, 8.4356e-01],\n",
      "        [7.3083e-02, 0.0000e+00, 9.2692e-01],\n",
      "        [1.0360e-03, 0.0000e+00, 9.9896e-01],\n",
      "        [2.3152e-04, 5.1354e-37, 9.9977e-01],\n",
      "        [9.4091e-04, 3.1950e-43, 9.9906e-01],\n",
      "        [6.8082e-06, 2.6367e-30, 9.9999e-01],\n",
      "        [6.1928e-03, 2.9662e-35, 9.9381e-01],\n",
      "        [1.3000e-03, 1.9358e-30, 9.9870e-01],\n",
      "        [1.8951e-03, 7.5669e-34, 9.9810e-01],\n",
      "        [1.4442e-04, 3.1113e-30, 9.9986e-01],\n",
      "        [5.9983e-07, 0.0000e+00, 1.0000e+00],\n",
      "        [9.3230e-05, 5.6534e-24, 9.9991e-01],\n",
      "        [1.7623e-02, 0.0000e+00, 9.8238e-01],\n",
      "        [7.4233e-03, 1.0843e-38, 9.9258e-01],\n",
      "        [2.8934e-03, 8.7052e-34, 9.9711e-01],\n",
      "        [4.8021e-03, 2.4598e-34, 9.9520e-01],\n",
      "        [1.5753e-04, 8.9675e-25, 9.9984e-01],\n",
      "        [1.7759e-03, 7.4345e-39, 9.9822e-01],\n",
      "        [1.5810e-04, 1.1561e-33, 9.9984e-01],\n",
      "        [2.1254e-02, 3.3631e-44, 9.7875e-01],\n",
      "        [4.8974e-05, 1.6064e-26, 9.9995e-01],\n",
      "        [2.6350e-03, 0.0000e+00, 9.9736e-01],\n",
      "        [2.9868e-04, 2.4242e-43, 9.9970e-01],\n",
      "        [2.2138e-03, 4.6780e-35, 9.9779e-01],\n",
      "        [1.3132e-03, 4.0459e-28, 9.9869e-01],\n",
      "        [8.8049e-03, 1.2341e-38, 9.9120e-01],\n",
      "        [3.5247e-04, 8.3555e-28, 9.9965e-01],\n",
      "        [1.9457e-03, 4.4292e-34, 9.9805e-01],\n",
      "        [8.3375e-05, 1.4907e-39, 9.9992e-01],\n",
      "        [4.5547e-02, 0.0000e+00, 9.5445e-01],\n",
      "        [1.0561e-03, 4.6341e-42, 9.9894e-01],\n",
      "        [2.2846e-02, 0.0000e+00, 9.7715e-01],\n",
      "        [3.7930e-03, 9.0769e-39, 9.9621e-01],\n",
      "        [8.8324e-03, 0.0000e+00, 9.9117e-01],\n",
      "        [3.3619e-03, 9.8091e-45, 9.9664e-01],\n",
      "        [5.0112e-02, 0.0000e+00, 9.4989e-01],\n",
      "        [2.2728e-03, 1.8445e-37, 9.9773e-01],\n",
      "        [1.5570e-04, 6.7767e-42, 9.9984e-01],\n",
      "        [3.1643e-04, 0.0000e+00, 9.9968e-01],\n",
      "        [4.6194e-03, 5.8100e-37, 9.9538e-01],\n",
      "        [2.6419e-04, 1.0546e-37, 9.9974e-01],\n",
      "        [4.6899e-04, 4.6372e-35, 9.9953e-01],\n",
      "        [1.8609e-01, 0.0000e+00, 8.1391e-01],\n",
      "        [4.5709e-01, 0.0000e+00, 5.4291e-01],\n",
      "        [1.5110e-02, 0.0000e+00, 9.8489e-01],\n",
      "        [8.1872e-01, 0.0000e+00, 1.8128e-01],\n",
      "        [7.2561e-01, 0.0000e+00, 2.7439e-01],\n",
      "        [5.2149e-02, 0.0000e+00, 9.4785e-01],\n",
      "        [8.7183e-01, 0.0000e+00, 1.2817e-01],\n",
      "        [2.1140e-04, 0.0000e+00, 9.9979e-01],\n",
      "        [3.1592e-02, 0.0000e+00, 9.6841e-01],\n",
      "        [3.8618e-01, 0.0000e+00, 6.1382e-01],\n",
      "        [7.4794e-01, 0.0000e+00, 2.5206e-01],\n",
      "        [1.5709e-02, 0.0000e+00, 9.8429e-01],\n",
      "        [5.5684e-05, 0.0000e+00, 9.9994e-01],\n",
      "        [5.0470e-01, 0.0000e+00, 4.9530e-01],\n",
      "        [6.7571e-02, 0.0000e+00, 9.3243e-01],\n",
      "        [5.9864e-02, 0.0000e+00, 9.4014e-01],\n",
      "        [5.7726e-03, 0.0000e+00, 9.9423e-01],\n",
      "        [3.3818e-04, 0.0000e+00, 9.9966e-01],\n",
      "        [2.2123e-01, 0.0000e+00, 7.7876e-01],\n",
      "        [2.0263e-05, 3.3096e-35, 9.9998e-01],\n",
      "        [5.7057e-04, 0.0000e+00, 9.9943e-01],\n",
      "        [1.1257e-04, 0.0000e+00, 9.9989e-01],\n",
      "        [4.5559e-08, 0.0000e+00, 1.0000e+00],\n",
      "        [1.3600e-04, 0.0000e+00, 9.9986e-01],\n",
      "        [8.1701e-03, 0.0000e+00, 9.9183e-01],\n",
      "        [4.5270e-05, 1.6220e-33, 9.9995e-01],\n",
      "        [4.1139e-04, 4.2205e-35, 9.9959e-01],\n",
      "        [8.6062e-03, 4.4454e-36, 9.9139e-01],\n",
      "        [1.2693e-03, 1.7133e-35, 9.9873e-01],\n",
      "        [1.0492e-03, 5.8773e-33, 9.9895e-01],\n",
      "        [2.3743e-05, 3.1789e-29, 9.9998e-01],\n",
      "        [4.4239e-04, 8.6426e-32, 9.9956e-01],\n",
      "        [4.2599e-03, 2.5223e-44, 9.9574e-01],\n",
      "        [2.3700e-04, 4.0029e-26, 9.9976e-01],\n",
      "        [3.2289e-03, 2.0931e-34, 9.9677e-01],\n",
      "        [3.2947e-04, 2.6414e-26, 9.9967e-01],\n",
      "        [7.8195e-06, 6.6988e-30, 9.9999e-01],\n",
      "        [8.5738e-06, 5.8590e-38, 9.9999e-01],\n",
      "        [2.4556e-04, 6.8928e-30, 9.9975e-01],\n",
      "        [4.8841e-03, 1.1604e-28, 9.9512e-01],\n",
      "        [9.3713e-04, 7.1952e-29, 9.9906e-01],\n",
      "        [3.0651e-06, 0.0000e+00, 1.0000e+00],\n",
      "        [3.4366e-03, 1.0229e-43, 9.9656e-01],\n",
      "        [1.9740e-03, 0.0000e+00, 9.9803e-01],\n",
      "        [3.2131e-03, 1.8661e-37, 9.9679e-01],\n",
      "        [7.4541e-04, 0.0000e+00, 9.9925e-01],\n",
      "        [1.7708e-02, 1.6339e-42, 9.8229e-01],\n",
      "        [2.0427e-06, 1.0171e-40, 1.0000e+00],\n",
      "        [5.5406e-06, 6.4215e-39, 9.9999e-01],\n",
      "        [6.5805e-04, 0.0000e+00, 9.9934e-01],\n",
      "        [1.2093e-04, 1.4013e-45, 9.9988e-01],\n",
      "        [2.2252e-02, 0.0000e+00, 9.7775e-01],\n",
      "        [5.1209e-04, 0.0000e+00, 9.9949e-01],\n",
      "        [1.3205e-03, 1.6816e-43, 9.9868e-01],\n",
      "        [2.4159e-03, 9.2626e-43, 9.9758e-01],\n",
      "        [9.8771e-04, 0.0000e+00, 9.9901e-01],\n",
      "        [1.9941e-04, 0.0000e+00, 9.9980e-01],\n",
      "        [1.4798e-04, 0.0000e+00, 9.9985e-01],\n",
      "        [7.1400e-05, 0.0000e+00, 9.9993e-01],\n",
      "        [1.2024e-03, 1.9425e-40, 9.9880e-01],\n",
      "        [5.3701e-03, 2.5709e-35, 9.9463e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Second is to define a pytorch model with softmax\n",
    "model_softmax = nn.Sequential(\n",
    "    nn.Linear(pytorch_features.shape[1], len(np.unique(pytorch_labels))),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Then pass the data through the model once without backpropagation\n",
    "outputs_softmax = model_softmax(pytorch_features)\n",
    "\n",
    "# Finally print out the outputs_softmax\n",
    "print(outputs_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83569c",
   "metadata": {},
   "source": [
    "(b) I unfortunately always encounter an error of dead kernel when trying to evaluate the train_and_val function. I don't really understand the root cause of the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8dd8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val(model,train_X,train_y,epochs,draw_curve=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------\n",
    "    model: a PyTorch model\n",
    "    train_X: np.array shape(ndata,nfeatures)\n",
    "    train_y: np.array shape(ndata)\n",
    "    epochs: int\n",
    "    draw_curve: bool\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Define your loss function, optimizer. Convert data to torch tensor ###\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "    \n",
    "    train_y -= 1\n",
    "    Xs = torch.tensor(train_X).float()\n",
    "    ys = torch.tensor(train_y).long()\n",
    "    \n",
    "    # Define Kfolds \n",
    "    kf = KFold(n_splits = 3,shuffle = True)\n",
    "    for train_index, test_index in kf.split(Xs):\n",
    "        train_X, test_X = Xs[train_index], Xs[test_index]\n",
    "        train_y, test_y = ys[train_index], ys[test_index]\n",
    "    \n",
    "    ### Split training examples further into training and validation ###\n",
    "    train_X,val_X,train_y,val_y = train_test_split(train_X,train_y, test_size = 0.20)\n",
    "    val_array=[]\n",
    "    lowest_val_loss = np.inf\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        ### Compute the loss and do backpropagation ###\n",
    "        optimizer.zero_grad()\n",
    "        train_out = model(train_X)\n",
    "        train_loss = loss(train_out, train_y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### compute validation loss and keep track of the lowest val loss ###\n",
    "        # compute validation loss\n",
    "        val_out = model(val_X) \n",
    "        val_loss = loss(val_out, val_y)\n",
    "        \n",
    "        # append val loss to val_array\n",
    "        val_array.append(val_loss.item())\n",
    "\n",
    "        # keep track of the lowest val loss\n",
    "        if val_loss < lowest_val_loss:\n",
    "            lowest_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "  \n",
    "    # The final number of epochs is when the minimum error in validation set occurs    \n",
    "   \n",
    "    final_epochs = np.argmin(val_array) + 1\n",
    "    print(\"Number of epochs with lowest validation:\",final_epochs)\n",
    "    ### Recover the model weight ###\n",
    "    model.load_state_dict(torch.load('model.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    ### Plot the validation loss curve ###\n",
    "\n",
    "    if draw_curve:\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(val_array))+1,val_array,label='Validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_val(model_softmax,pytorch_features,pytorch_labels,1000,draw_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08424d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
