# -*- coding: utf-8 -*-
"""Copy of Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14pTqt0zG10q_jlkv-tmTbIh-nOXS09Ek
"""

import numpy as np 
import pandas as pd
import math 
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split, KFold

class NaiveBayesClassifier():
    def __init__(self):
        self.type_indices={}    # store the indices of wines that belong to each cultivar as a boolean array of length 178
        self.type_stats={}      # store the mean and std of each cultivar
        self.ndata = 0
        self.trained=False
    
    @staticmethod
    def gaussian(x,mean,std):
        exponent = -(x - mean)**2 / (2 * std**2)
        
        return (np.exp(exponent) / (np.sqrt(2 * np.pi) * std))
    
    @staticmethod
    def calculate_statistics(x_values):
        # Returns a list with length of input features. Each element is a tuple, with the input feature's average and standard deviation
        n_feats=x_values.shape[1]
        return [(np.average(x_values[:,n]),np.std(x_values[:,n])) for n in range(n_feats)]
    
    @staticmethod
    def calculate_prob(x_input,stats):
        """Calculate the probability that the input features belong to a specific class(P(X|C)), defined by the statistics of features in that class
        x_input: np.array shape(nfeatures)
        stats: list of tuple [(mean1,std1),(means2,std2),...]
        """ 
        init_prob = 1 
        for i in range(len(x_input)):
            mean, std = stats[i]
            init_prob *= NaiveBayesClassifier.gaussian(x_input[i], mean, std)
        return init_prob
    
    def fit(self,xs,ys):
        # Train the classifier by calculating the statistics of different features in each class
        self.ndata = len(ys)
        for y in set(ys):
            type_filter= (ys==y)
            self.type_indices[y]=type_filter
            self.type_stats[y]=self.calculate_statistics(xs[type_filter])
        self.trained=True
            
    def predict(self,xs):
        # Do the prediction by outputing the class that has highest probability
        if len(xs.shape)>1:
            print("Only accepts one sample at a time!")
        if self.trained:
            guess=None
            max_prob=0
            # P(C|X) = P(X|C)*P(C) / sum_i(P(X|C_i)*P(C_i)) (deniminator for normalization only, can be ignored)
            for y_type in self.type_stats:
                pre = sum(self.type_indices[y_type]) / self.ndata
                prob= self.calculate_prob(xs, self.type_stats[y_type]) * pre
                if prob>max_prob:
                    max_prob=prob
                    guess=y_type
            return guess
        else:
            print("Please train the classifier first!")
            
def calculate_accuracy(model,xs,ys):
    y_pred=np.zeros_like(ys)
    for idx,x in enumerate(xs):
        y_pred[idx]=model.predict(x)
    return np.sum(ys==y_pred)/len(ys)

# Import wines.csv
wines = pd.read_csv('wines.csv')
wines.head()

# Define the instance from the Naive Bayes Classifier
nbc = NaiveBayesClassifier()

# Fit the Naive Bayes Classifier
nbc.fit(wines.loc[:, 'Alcohol %':'Proline'].values, wines.loc[:, 'ranking'].values)

# First get the stats for cultivar 1
type_stats = nbc.type_stats[1]

# Then calculate the probability of alcohol% = 13
probability = nbc.gaussian(13, type_stats[0][0], type_stats[0][1])

print(f"A wine from cultivar 1 has a {round(probability*100, 2)}% probability of containinh 13% Alcohol")

# First normalize the wines dataframe
wines_norm = wines.loc[:, 'Alcohol %':'Proline']
wines_norm = (wines_norm - np.mean(wines_norm, axis=0)) / np.std(wines_norm, axis=0)
wines_norm = wines_norm.merge(wines[['Start assignment','ranking']], left_index=True, right_index=True)
wines_norm.head()

# Divide the normalized wines data into 3-fold training and testing groups
# and use 2/3 training and 1/3 testing for the three divisions
kf = KFold(n_splits=3, shuffle=True)
xs = wines.loc[:, 'Alcohol %':'Proline'].values 
ys = wines.loc[:, 'ranking'].values
nbc = NaiveBayesClassifier()
accuracy = []

for train_index, test_index in kf.split(xs):
    x_train, x_test = xs[train_index], xs[test_index]
    y_train, y_test = ys[train_index], ys[test_index]

    # train the classifier
    nbc.fit(x_train,y_train)
    accuracy.append(calculate_accuracy(nbc,x_test,y_test))
    print(f'Accuracy: {calculate_accuracy(nbc,x_test,y_test)}')
print(f'Average accuracy after 3-fold training is {np.array(accuracy).mean()}')

# First convert the features and labels to PyTorch tensors
pytorch_features = torch.tensor(wines.loc[:, 'Alcohol %':'Proline'].values , dtype=torch.float32)
pytorch_labels = torch.tensor(wines.loc[:, 'ranking'].values, dtype=torch.int64)

# Then define a pytorch model without softmax
model_no_softmax = nn.Sequential(
    nn.Linear(pytorch_features.shape[1], len(np.unique(pytorch_labels)))
)

# Then pass the data through the model once without backpropagation
outputs_no_softmax = model_no_softmax(pytorch_features)

# Finally print out the outputs_no_softmax
print(outputs_no_softmax)

# Second is to define a pytorch model with softmax
model_softmax = nn.Sequential(
    nn.Linear(pytorch_features.shape[1], len(np.unique(pytorch_labels))),
    nn.Softmax(dim=1)
)

# Then pass the data through the model once without backpropagation
outputs_softmax = model_softmax(pytorch_features)

# Finally print out the outputs_softmax
print(outputs_softmax)

def train_and_val(model, train_X, train_y, epochs, draw_curve=True):
    """
    Train and validate a PyTorch model using cross-entropy loss.
    
    Parameters
    ----------
    model : PyTorch model
        The model to train.
    train_X : numpy.ndarray
        The input training data of shape (n_samples, n_features).
    train_y : numpy.ndarray
        The target training data of shape (n_samples,).
    epochs : int
        The number of training epochs.
    draw_curve : bool, optional
        Whether to draw a validation loss curve (the default is True).
    
    Returns
    -------
    float
        The final validation loss.
    """
    # Convert data to torch tensor
    Xs = torch.tensor(train_X).float()
    ys = torch.tensor(train_y).long()
    
    # Define Kfolds 
    kf = KFold(n_splits=3, shuffle=True)
    for train_index, test_index in kf.split(Xs):
        train_X, test_X = Xs[train_index], Xs[test_index]
        train_y, test_y = ys[train_index], ys[test_index]
    
        # Subtract one from labels to make them 0-based
        train_y -= 1
        test_y -= 1
    
        # Split training examples further into training and validation
        train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.2)
        
        # Define loss function and optimizer
        loss_fn = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        # Keep track of the lowest validation loss
        lowest_val_loss = np.inf
        
        # Train the model
        val_array = []
        for epoch in range(epochs):
            # Compute training loss and update model parameters
            optimizer.zero_grad()
            train_out = model(train_X)
            train_loss = loss_fn(train_out, train_y)
            train_loss.backward()
            optimizer.step()
            
            # Compute validation loss and keep track of the lowest validation loss
            val_out = model(val_X)
            val_loss = loss_fn(val_out, val_y)
            val_array.append(val_loss.item())
            if val_loss < lowest_val_loss:
                lowest_val_loss = val_loss
                torch.save(model.state_dict(), 'model.pt')
        
        # The final number of epochs is when the minimum error in validation set occurs    
        final_epochs = np.argmin(val_array) + 1
        print("Number of epochs with lowest validation:", final_epochs)
        
        # Recover the model weight
        model.load_state_dict(torch.load('model.pt'))
        model.eval()
        
        # Compute test accuracy
        test_out = model(test_X)
        test_loss = loss_fn(test_out, test_y)
        test_preds = torch.argmax(test_out, dim=1).numpy()
        test_acc = np.mean(test_preds == test_y.numpy())
        print("Test accuracy:", test_acc)
        
        # Plot the validation loss curve
        if draw_curve:
            plt.figure()
            plt.plot(np.arange(len(val_array)) + 1, val_array, label='Validation loss')
            plt.xlabel('Epochs')
            plt.ylabel('Loss')
            plt.legend()
            
    return lowest_val_loss.item()

train_and_val(model_no_softmax, pytorch_features, pytorch_labels, 1000, draw_curve=True)

train_and_val(model_softmax, pytorch_features, pytorch_labels, 1000, draw_curve=True)

class Classifier_softmax(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(Classifier_softmax, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        out = self.fc1(x)
        out = nn.functional.relu(out)
        out = self.fc2(out)
        out = nn.functional.softmax(out, dim=1)
        return out

wine_classifier = Classifier_softmax(pytorch_features.shape[1], len(np.unique(pytorch_labels)), 3)

train_and_val(wine_classifier, pytorch_features, pytorch_labels, 1000, draw_curve=True)

class Classifier_Relu(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(Classifier_Relu, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

wine_classifier_2 = Classifier_Relu(pytorch_features.shape[1], len(np.unique(pytorch_labels)), 3)

train_and_val(wine_classifier_2, pytorch_features, pytorch_labels, 1000, draw_curve=True)