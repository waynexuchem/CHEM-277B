{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7ce482",
   "metadata": {},
   "source": [
    "# Chem277B: Machine Learning Algorithms\n",
    "\n",
    "## Homework assignment #7: Deeper Learning and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c78bfa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "from functools import wraps\n",
    "from time import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbc70d",
   "metadata": {},
   "source": [
    "### 1. Bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63e3ef",
   "metadata": {},
   "source": [
    "(a) I have sorted out the training and testing datasets and normalized the data using each 32 * 32 image's maximum pixel value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fdb381e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 32, 32), y=(60000,)\n",
      "Test: X=(10000, 32, 32), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "# First load the mnist data and convert all elements into lists / arrays\n",
    "mnist = list(pd.read_pickle('mnist.pkl'))\n",
    "mnist[0] = list(mnist[0])\n",
    "mnist[1] = list(mnist[1])\n",
    "train_X = mnist[0][0]\n",
    "train_y = mnist[0][1]\n",
    "test_X = mnist[1][0]\n",
    "test_y = mnist[1][1]\n",
    "print('Train: X=%s, y=%s' % (train_X.shape, train_y.shape))\n",
    "print('Test: X=%s, y=%s' % (test_X.shape, test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "990126da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to normalize the training and testing data sets\n",
    "def normalize_pixels(train_X, test_X):\n",
    "    \n",
    "    # First convert the dataset to floats\n",
    "    train_X_norm = train_X.astype('float32')\n",
    "    test_X_norm = test_X.astype('float32')\n",
    "    \n",
    "    # Find maximum values for all 60000 / 10000 pictures in train and test datasets\n",
    "    # and broadcast to a (60000 / 10000, 32, 32) shape array\n",
    "    train_X_max = np.broadcast_to(train_X.max(axis=(1,2))[:, np.newaxis, np.newaxis], (60000, 32, 32))\n",
    "    test_X_max = np.broadcast_to(test_X.max(axis=(1,2))[:, np.newaxis, np.newaxis], (10000, 32, 32))\n",
    "    \n",
    "    # Normalize the datasets\n",
    "    train_X_norm = train_X_norm / train_X_max\n",
    "    test_X_norm = test_X_norm / test_X_max\n",
    "    \n",
    "    # Return the normalized datasets\n",
    "    return train_X_norm, test_X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "38f220e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567298545 6148662.5\n",
      "264923200 1039329.2\n"
     ]
    }
   ],
   "source": [
    "# Normalize the datasets and confirm the data has been normalized\n",
    "train_X_norm, test_X_norm = normalize_pixels(train_X, test_X)\n",
    "print(train_X.sum(), train_X_norm.sum())\n",
    "print(test_X.sum(), test_X_norm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d1649",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e895cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print('func:%r  took: %2.4f sec' % (f.__name__,  te-ts))\n",
    "        return result\n",
    "    return wrap\n",
    "\n",
    "def create_chunks(complete_list, chunk_size=None, num_chunks=None):\n",
    "    '''\n",
    "    Cut a list into multiple chunks, each having chunk_size (the last chunk might be less than chunk_size)\n",
    "    or having a total of num_chunk chunks\n",
    "    '''\n",
    "    chunks = []\n",
    "    if num_chunks is None:\n",
    "        num_chunks = math.ceil(len(complete_list) / chunk_size)\n",
    "    elif chunk_size is None:\n",
    "        chunk_size = math.ceil(len(complete_list) / num_chunks)\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(complete_list[i * chunk_size: (i + 1) * chunk_size])\n",
    "    return chunks\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer_type, learning_rate, epoch, batch_size, input_transform=lambda x: x,):\n",
    "        \"\"\" The class for training the model\n",
    "        model: nn.Module\n",
    "            A pytorch model\n",
    "        optimizer_type: 'adam' or 'sgd'\n",
    "        learning_rate: float\n",
    "        epoch: int\n",
    "        batch_size: int\n",
    "        input_transform: func\n",
    "            transforming input. Can do reshape here\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        if optimizer_type == \"sgd\":\n",
    "            self.optimizer = SGD(model.parameters(), learning_rate,momentum=0.9)\n",
    "        elif optimizer_type == \"adam\":\n",
    "            self.optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "            \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "    @timing\n",
    "    def train(self, inputs, outputs, val_inputs, val_outputs, early_stop=False, l2=False, silent=False):\n",
    "        \"\"\" train self.model with specified arguments using 3-fold cross-validation\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        val_nputs: np.array, The shape of input_transform(val_input) should be (ndata,nfeatures)\n",
    "        val_outputs: np.array shape (ndata,)\n",
    "        early_stop: bool\n",
    "        l2: bool\n",
    "        silent: bool. Controls whether or not to print the train and val error during training\n",
    "        \n",
    "        @return\n",
    "        a dictionary of arrays with train and val losses and accuracies\n",
    "        \"\"\"\n",
    "        ### convert data to tensor of correct shape and type here ###\n",
    "        inputs = torch.Tensor(self.input_transform(inputs)).float()\n",
    "        outputs = torch.Tensor(outputs).long()\n",
    "        \n",
    "        # Split data into 3-fold groups of training (2/3 of the training data), \n",
    "        # validation (1/3 of the training data)\n",
    "        inputs_train, inputs_val, outputs_train, outputs_val = [], [], [], []\n",
    "        for i in range(3):\n",
    "            inputs_train_fold, inputs_val_fold, outputs_train_fold, outputs_val_fold = \\\n",
    "                train_test_split(inputs, outputs, test_size=1/3, random_state=i)\n",
    "            inputs_train.append(inputs_train_fold)\n",
    "            inputs_val.append(inputs_val_fold)\n",
    "            outputs_train.append(outputs_train_fold)\n",
    "            outputs_val.append(outputs_val_fold)\n",
    "        \n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        weights = self.model.state_dict()\n",
    "        lowest_val_loss = np.inf\n",
    "        \n",
    "        for n_epoch in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            batch_indices = list(range(inputs.shape[0]))\n",
    "            random.shuffle(batch_indices)\n",
    "            batch_indices = create_chunks(batch_indices, chunk_size=self.batch_size)\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for batch in batch_indices:\n",
    "                batch_importance = len(batch) / len(outputs)\n",
    "                batch_input = inputs_train[batch]\n",
    "                batch_output = outputs_train[batch]\n",
    "                ### make prediction and compute loss with loss function of your choice on this batch ###\n",
    "                batch_predictions = self.model(batch_input)\n",
    "                loss = F.cross_entropy(batch_predictions, batch_output)\n",
    "                if l2:\n",
    "                    ### Compute the loss with L2 regularization ###\n",
    "                    l2_lambda = 0.01\n",
    "                    l2_reg = torch.tensor(0.)\n",
    "                    for param in self.model.parameters():\n",
    "                        l2_reg += torch.norm(param)\n",
    "                    loss += l2_lambda * l2_reg\n",
    "                    \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                ### Compute epoch_loss and epoch_acc\n",
    "                epoch_loss += loss.item() * batch_importance\n",
    "                epoch_acc += (batch_predictions.argmax(dim=1) == batch_output).float().mean().item() * batch_importance\n",
    "            \n",
    "            val_loss, val_acc = self.evaluate(val_inputs[0], val_outputs[0], print_acc=False)\n",
    "            for i in range(1, 3):\n",
    "                val_loss_i, val_acc_i = self.evaluate(inputs_val[i], outputs_val[i], print_acc=False)\n",
    "                val_loss += val_loss_i\n",
    "                val_acc += val_acc_i\n",
    "            val_loss /= 3\n",
    "            val_acc /= 3\n",
    "            \n",
    "            if n_epoch % 10 ==0 and not silent: \n",
    "                print(\"Epoch %d/%d - Loss: %.3f - Acc: %.3f\" % (n_epoch + 1, self.epoch, epoch_loss, epoch_acc))\n",
    "                print(\"              Val_loss: %.3f - Val_acc: %.3f\" % (val_loss, val_acc))\n",
    "            losses.append(epoch_loss)\n",
    "            accuracies.append(epoch_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            if early_stop:\n",
    "                if val_loss < lowest_val_loss:\n",
    "                    lowest_val_loss = val_loss\n",
    "                    weights = self.model.state_dict()\n",
    "\n",
    "        if early_stop:\n",
    "            self.model.load_state_dict(weights)    \n",
    "\n",
    "        return {\"losses\": losses, \"accuracies\": accuracies, \"val_losses\": val_losses, \"val_accuracies\": val_accuracies}\n",
    "        \n",
    "    def evaluate(self, inputs, outputs, print_acc=True):\n",
    "        \"\"\" evaluate model on provided input and output\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        print_acc: bool\n",
    "        \n",
    "        @return\n",
    "        losses: float\n",
    "        acc: float\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            inputs = torch.tensor(self.input_transform(inputs)).float()\n",
    "            outputs = torch.tensor(outputs).long()\n",
    "            outputs_pred = self.model(inputs)\n",
    "            losses = F.cross_entropy(outputs_pred, outputs).items()\n",
    "            acc = torch.mean((torch.argmax(outputs_pred, dim=1) == outputs).float()).items()\n",
    "        if print_acc:\n",
    "            print(\"Accuracy: %.3f\" % acc)\n",
    "        return losses, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc067da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
