{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1190ac1b",
   "metadata": {},
   "source": [
    "# Chem277B: Machine Learning Algorithms\n",
    "\n",
    "## Homework assignment #3: Meta-heuristic algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc35ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import time\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5523d",
   "metadata": {},
   "source": [
    "### 1. Genetic Algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0430263",
   "metadata": {},
   "source": [
    "(a) The solutions for encodings A and B, as well as their schema are listed below:\n",
    "\n",
    "Encoding A Solutions | Fitness | Vector | Schema\n",
    "--- | --- | --- | ---\n",
    "x = 3 | 30 | 1000 | $*0**$\n",
    "x = 4 | 31 | 0010 | $*0**$\n",
    "x = 5 | 30 | 0001 | $*0**$\n",
    "\n",
    "Encoding B Solutions | Fitness | Vector | Schema\n",
    "--- | --- | --- | ---\n",
    "x = 3 | 30 | 1101 | $1**1$\n",
    "x = 4 | 31 | 1011 | $1**1$\n",
    "x = 5 | 30 | 1111 | $1**1$\n",
    "\n",
    "The length and order of the two schema are shown below:\n",
    "\n",
    "Encoding | Schema | Length | Order\n",
    "--- | --- | --- | ---\n",
    "Encoding A | $*0**$ | 0 | 1\n",
    "Encoding B | $1**1$ | 3 | 2\n",
    "\n",
    "Given that the principle of schema should be lower length and lower order, I will choose encoding A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eecb9d",
   "metadata": {},
   "source": [
    "(b) The listed solutions and fitness and their grouping are below:\n",
    "\n",
    "Candidate Solutions | fitness | Encoding | Pairing Group\n",
    "--- | --- | --- | ---\n",
    "x = 10 | -5 | 0101 | Group 2\n",
    "x = 1 | 22 | 0011 | Group 2\n",
    "x = 15 | -90 | 1111 | Group 1 (least fit)\n",
    "x = 6 | 27 | 0000 | Group 1 (fittest)\n",
    "x = 0 | 15 | 1011 | Group 3\n",
    "x = 9 | 6 | 1100 | Group 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7a451",
   "metadata": {},
   "source": [
    "(c) After cross-over operation, the new encodings are as follow:\n",
    "\n",
    "Group | Candidate Solutions | Original fitness | Encoding | After cross-over | New fitness | New solutions\n",
    "--- | --- | --- | --- | --- | --- | ---\n",
    "1 | x = 15 | -90 | 1\\|111 | 1*000* | 30 | x = 3\n",
    "1 | x = 6 | 27 | 0\\|000 | 0*111* | -33 | x = 12\n",
    "2 | x = 10 | -5 | 0\\|101 | 0*011* | 22 | x = 1\n",
    "2 | x = 1 | 22 | 0\\|011 | 0*101* | -5 | x = 10\n",
    "3 | x = 0 | 15 | 1\\|011 | 1*100* | 6 | x = 9\n",
    "3 | x = 9 | 6 | 1\\|100 | 1*011* | 15 | x = 0\n",
    "--- | Sum fitness | -25 | --- | --- | 35 | ---\n",
    "\n",
    "For group 1 with the fittest and least fit pairs, the original fitness are -90 and 27. Now the fitness after cross-over operation are 30 and -33. I consider the fitness increased.\n",
    "\n",
    "For group 2 the fitness didn't change. They remain as 22 and -5.\n",
    "\n",
    "For group 3 the fitness didn't change. They remain as 6 and 15.\n",
    "\n",
    "Group 1 still has the highest fitness score. Total fitness of the new group after cross-over operation increased from -25 to 35. The population increased as a whole. The best solution is encoding 1000 with a fitness of 30. Now the solution x = 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a04ea61",
   "metadata": {},
   "source": [
    "(d) After mutation, the new encodings, the new solutions and their corresponding fitness are shown below:\n",
    "\n",
    "Group | Solutions after cross-over | fitness | Encoding | Encoding after mutation | New solutions | New fitness\n",
    "--- | --- | --- | --- | --- | --- | ---\n",
    "1 | x = 3 | 30 | 10[$0$]0 | 10*1*0 | x = 7 | 22\n",
    "1 | x = 12 | -33 | 01[$1$]1 | 01*0*1 | x = 10 | -5\n",
    "2 | x = 1 | 22 | 00[$1$]1 | 00*0*1 | x = 5 | 30\n",
    "2 | x = 10 | -5 | 01[$0$]1 | 01*1*1 | x = 12 | -33\n",
    "3 | x = 9 | 6 | 11[$0$]0 | 11*1*0 | x = 14 | -69\n",
    "3 | x = 0 | 15 | 10[$1$]1 | 10*0*1 | x = 2 | 27\n",
    "--- | Sum fitness | 35 | --- | --- | --- | -28\n",
    "\n",
    "It seems mutation didn't increase the total fitness of the population. We found a solution of x = 5 with fitness of 30, same fitness as before mutation. It's not a better solution in my opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8988838",
   "metadata": {},
   "source": [
    "(e) The least fit member after step d is in group 3, entry 5, x = 14 that leads to a fitness of -69. \n",
    "\n",
    "The best fit member is in group 2, entry 3, x = 5 that leads to a fitness of 30.\n",
    "\n",
    "We will perform cloning then followed by 2-point cross-over as shown below, with grouping remaining the same since there are now two fittest entries and no instruction to regroup:\n",
    "\n",
    "Group | Solutions after cloning | fitness | Encoding | Encoding after 2-point cross-over | New solutions | New fitness\n",
    "--- | --- | --- | --- | --- | --- | ---\n",
    "1 | x = 7 | 22 | 1*[01]*0 | 1*10*0 | x = 9 | 6\n",
    "1 | x = 10 | -5 | 0*[10]*1 | 0*01*1 | x = 1 | 22\n",
    "2 | x = 5 | 30 | 0*[00]*1 | 0*11*1 | x = 12 | -33\n",
    "2 | x = 12 | -33 | 0*[11]*1 | 0*00*1 | x = 5 | 30\n",
    "3 | x = 14 -> x = 5 | -69 -> 30 | 1110 -> 0*[00]*1 | 0*00*1 | x = 5 | 30\n",
    "3 | x = 2 | 27 | 1*[00]*1 | 1*00*1 | x = 2 | 27\n",
    "--- | Sum fitness | 71 | --- | --- | --- | 82\n",
    "\n",
    "Now we can see the new solution is clearly better. We now have a total fitness of 82. There are two fittest solutions still, both of which are x = 5 and fitness of 30. We don't have a better solution than before in this case though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7313ea",
   "metadata": {},
   "source": [
    "(f) We will perform cloning then followed by cross-over as shown below:\n",
    "\n",
    "Group | Solutions after cloning | fitness | Encoding | Encoding after cross-over | New solutions | New fitness\n",
    "--- | --- | --- | --- | --- | --- | ---\n",
    "1 | x = 9 | 6 | [110]\\|0 | 0010 | x = 4 | 31\n",
    "1 | x = 1 | 22 | [001]\\|1 | 1101 | x = 13 | -50\n",
    "2 | x = 12 -> x = 5 | -33 -> 30 | 0111 -> [000]\\|1 | 0001 | x = 5 | 30\n",
    "2 | x = 5 | 30 | [000]\\|1 | 0001 | x = 5 | 30\n",
    "3 | x = 5 | 30 | [000]\\|1 | 1001 | x = 2 | 27\n",
    "3 | x = 2 | 27 | [100]\\|1 | 0001 | x = 5 | 30\n",
    "--- | Sum fitness | 82 -> 145 | --- | --- | --- | 98\n",
    "\n",
    "The result is good. We not only increased the total fitness, but also found the best solution to the fitting equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031d4c1",
   "metadata": {},
   "source": [
    "(g) I think the encoding of the solution space is adequate, in the sense that it was able to find the best solution of the fitting space with a genetic algorithm style evolution, in a finite number of steps. The algorithm converged in short time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c7988",
   "metadata": {},
   "source": [
    "### 2. Artificial Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54da64e",
   "metadata": {},
   "source": [
    "We first define an artificial neural network class as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "29f4aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, architecture, learning_rate, activation):\n",
    "        # initialize the model\n",
    "        self.architecture = architecture     # Structure of NN, in HW case, [6, 2, 2] list\n",
    "        self.activation = activation         # Activation function, defined separately outside the class\n",
    "        self.learning_rate = learning_rate   # Learning rate lambda is a constant\n",
    "        self.layer = len(self.architecture)  # Number of layers of the NN\n",
    "        \n",
    "    def init_weight(self):\n",
    "        self.weights = []     # list of matrices for each layer\n",
    "        self.biases = []      # list of derivatives\n",
    "        for i in range(self.layer - 1):\n",
    "            prev_layer_num = self.architecture[i]\n",
    "            current_layer_num = self.architecture[i+1]\n",
    "            # The biases and weights for the network are initialized randomly with normal distribution\n",
    "            self.weights.append(np.random.random((current_layer_num, prev_layer_num)))\n",
    "            self.biases.append(np.random.random(current_layer_num))\n",
    "            \n",
    "    def feed_forward(self, a):\n",
    "        self.z_s = []\n",
    "        self.a_s = [a]\n",
    "        for i in range(self.layer - 1):\n",
    "            z_i = self.weights[i].dot(self.a_s[i]) + self.biases[i]\n",
    "            a_i = self.activation(z_i)\n",
    "        \n",
    "            # list of numpy arrays\n",
    "            self.z_s.append(z_i)      \n",
    "            self.a_s.append(a_i)\n",
    "        return self.a_s[-1]\n",
    "    \n",
    "    def calc_error(self, y, activation_grad):\n",
    "        self.error = self.a_s[-1] - y\n",
    "        self.weights_grad = [0] * (self.layer - 1)\n",
    "        self.biases_grad = [0] * (self.layer - 1)\n",
    "        return self.error * activation_grad(y)\n",
    "        \n",
    "    def calc_grad(self, y, activation_grad):\n",
    "        self.biases_grad = [np.zeros(b.shape) for b in self.biases]\n",
    "        self.weights_grad = [np.zeros(w.shape) for w in self.weights]\n",
    "        sp = activation_grad(y)\n",
    "        delta = self.error * sp[-1]\n",
    "        self.weights_grad[-1] = np.outer(delta, self.a_s[-2])\n",
    "        self.biases_grad[-1] = delta\n",
    "        for i in range(2, self.layer):\n",
    "            delta = np.dot(self.weights[-i+1].T, delta) * sp[-i]\n",
    "            self.weights_grad[-i] = np.outer(delta, self.a_s[-i-1])\n",
    "            self.biases_grad[-i] = delta\n",
    "    \n",
    "    def back_prop(self, activation_grad):\n",
    "        # calculate the gradient of the output layer\n",
    "        delta = self.calc_error(y, activation_grad) * self.activation(self.z_s[-1])\n",
    "        self.weights_grad[-1] = np.outer(delta, self.a_s[-2])\n",
    "        self.biases_grad[-1] = delta\n",
    "        # calculate the gradients of the hidden layers, from the back to the front\n",
    "        for l in range(2, self.layer):\n",
    "            z = self.z_s[-l]\n",
    "            sp = self.activation(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            self.weights_grad[-l] = np.outer(delta, self.a_s[-l-1])\n",
    "            self.biases_grad[-l] = delta\n",
    "        # update the weights and biases\n",
    "        for i in range(self.layer - 1):\n",
    "            self.weights[i] -= self.learning_rate * self.weights_grad[i]\n",
    "            self.biases[i] -= self.learning_rate * self.biases_grad[i]\n",
    "            \n",
    "    def fit(self, x, y, activation_grad):                  # Iterate the NN \n",
    "        self.feed_forward(x)\n",
    "        self.errors = self.calc_error(y, activation_grad)                \n",
    "        self.calc_grad(y, activation_grad)\n",
    "        self.back_prop(activation_grad)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f649e01f",
   "metadata": {},
   "source": [
    "(a) After initializing, the weights and biases of the neural network are as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "afca528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ,\n",
       "         0.64589411],\n",
       "        [0.43758721, 0.891773  , 0.96366276, 0.38344152, 0.79172504,\n",
       "         0.52889492]]),\n",
       " array([[0.07103606, 0.0871293 ],\n",
       "        [0.0202184 , 0.83261985]])]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "network = NN([6,2,2], 0.1, np.tanh)\n",
    "network.init_weight()\n",
    "network.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf8bbd",
   "metadata": {},
   "source": [
    "(b) With an input of [-1, 1, -1, -1, 1, -1], the fitting is [0.64299999 0.79969983], and the predicted secondary structure would be hydrophobic helix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1ecae182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized prediction: [0.64299999 0.79969983]\n"
     ]
    }
   ],
   "source": [
    "x = [-1, 1, -1, -1, 1, -1]\n",
    "print(\"Initialized prediction:\", network.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e3ecd",
   "metadata": {},
   "source": [
    "(c) The calculated errors for the hidden layer nodes are calculated as follow: [0.6305039  0.66240387].\n",
    "The prediction after fitting once is: [0.47417151 0.53893351]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7192f214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in nodes [0.6305039  0.66240387]\n",
      "Prediction after fitting once:  [0.47417151 0.53893351]\n"
     ]
    }
   ],
   "source": [
    "y = [-1, -1]\n",
    "def tanh_grad(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "network.fit(x, y, tanh_grad)\n",
    "errors = network.calc_error(y, tanh_grad)\n",
    "print(\"Error in nodes\", errors)\n",
    "print(\"Prediction after fitting once: \", network.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a405884d",
   "metadata": {},
   "source": [
    "(d) The general formula for weight updates is as follow:\n",
    "\n",
    "w' = w - learning_rate * activation_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
