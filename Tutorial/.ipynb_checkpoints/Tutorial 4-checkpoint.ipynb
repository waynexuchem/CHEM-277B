{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline \n",
    "\n",
    "* sklearn \n",
    "    * StandardScaler\n",
    "    * Train-test split and K-fold cross validation\n",
    "    * One-hot Encoder\n",
    "    * Multivariate linear regression\n",
    "* Q&A on HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize your dataset using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol %</th>\n",
       "      <th>Malic Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alkalinity</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Phenols.1</th>\n",
       "      <th>Proantho-cyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280 315</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Start assignment</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.12</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.32</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1280</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol %  Malic Acid   Ash  Alkalinity   Mg  Phenols  Flavanoids  \\\n",
       "0      14.23        1.71  2.43        15.6  127      2.8        3.06   \n",
       "1      13.24        2.59  2.87        21.0  118      2.8        2.69   \n",
       "2      14.83        1.64  2.17        14.0   97      2.8        2.98   \n",
       "3      14.12        1.48  2.32        16.8   95      2.2        2.43   \n",
       "4      13.75        1.73  2.41        16.0   89      2.6        2.76   \n",
       "\n",
       "   Phenols.1  Proantho-cyanins  Color intensity   Hue  OD280 315  Proline  \\\n",
       "0       0.28              2.29             5.64  1.04       3.92     1065   \n",
       "1       0.39              1.82             4.32  1.04       2.93      735   \n",
       "2       0.29              1.98             5.20  1.08       2.85     1045   \n",
       "3       0.26              1.57             5.00  1.17       2.82     1280   \n",
       "4       0.29              1.81             5.60  1.15       2.90     1320   \n",
       "\n",
       "   Start assignment  ranking  \n",
       "0                 1        1  \n",
       "1                 1        1  \n",
       "2                 1        1  \n",
       "3                 1        1  \n",
       "4                 1        1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"wines.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HW2, we normalize our data by our own code. This is how I did it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol %</th>\n",
       "      <th>Malic Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alkalinity</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Phenols.1</th>\n",
       "      <th>Proantho-cyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280 315</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.259772</td>\n",
       "      <td>-0.625086</td>\n",
       "      <td>-0.718336</td>\n",
       "      <td>-1.650049</td>\n",
       "      <td>-0.192495</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.954502</td>\n",
       "      <td>-0.578985</td>\n",
       "      <td>0.681738</td>\n",
       "      <td>0.061386</td>\n",
       "      <td>0.537671</td>\n",
       "      <td>0.336606</td>\n",
       "      <td>0.949319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.382733</td>\n",
       "      <td>-0.768712</td>\n",
       "      <td>-0.170035</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>-0.332922</td>\n",
       "      <td>-0.152402</td>\n",
       "      <td>0.402320</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.036617</td>\n",
       "      <td>-0.025128</td>\n",
       "      <td>0.932531</td>\n",
       "      <td>0.294232</td>\n",
       "      <td>1.697675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.925685</td>\n",
       "      <td>-0.544297</td>\n",
       "      <td>0.158946</td>\n",
       "      <td>-1.049479</td>\n",
       "      <td>-0.754202</td>\n",
       "      <td>0.488531</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.578985</td>\n",
       "      <td>0.383884</td>\n",
       "      <td>0.234414</td>\n",
       "      <td>0.844785</td>\n",
       "      <td>0.407228</td>\n",
       "      <td>1.825055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.493343</td>\n",
       "      <td>2.031997</td>\n",
       "      <td>1.803849</td>\n",
       "      <td>1.653086</td>\n",
       "      <td>0.860705</td>\n",
       "      <td>-0.504914</td>\n",
       "      <td>-1.073511</td>\n",
       "      <td>-0.740141</td>\n",
       "      <td>-0.842575</td>\n",
       "      <td>1.488867</td>\n",
       "      <td>-1.261138</td>\n",
       "      <td>-0.976966</td>\n",
       "      <td>-0.372246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.332758</td>\n",
       "      <td>1.744744</td>\n",
       "      <td>-0.389355</td>\n",
       "      <td>0.151661</td>\n",
       "      <td>1.422412</td>\n",
       "      <td>-1.129824</td>\n",
       "      <td>-1.344582</td>\n",
       "      <td>0.549108</td>\n",
       "      <td>-0.422075</td>\n",
       "      <td>2.224236</td>\n",
       "      <td>-1.612125</td>\n",
       "      <td>-1.485445</td>\n",
       "      <td>0.280575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.209232</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>0.012732</td>\n",
       "      <td>0.151661</td>\n",
       "      <td>1.422412</td>\n",
       "      <td>-1.033684</td>\n",
       "      <td>-1.354622</td>\n",
       "      <td>1.354888</td>\n",
       "      <td>-0.229346</td>\n",
       "      <td>1.834923</td>\n",
       "      <td>-1.568252</td>\n",
       "      <td>-1.400699</td>\n",
       "      <td>0.296498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.395086</td>\n",
       "      <td>1.583165</td>\n",
       "      <td>1.365208</td>\n",
       "      <td>1.502943</td>\n",
       "      <td>-0.262708</td>\n",
       "      <td>-0.392751</td>\n",
       "      <td>-1.274305</td>\n",
       "      <td>1.596623</td>\n",
       "      <td>-0.422075</td>\n",
       "      <td>1.791666</td>\n",
       "      <td>-1.524378</td>\n",
       "      <td>-1.428948</td>\n",
       "      <td>-0.595160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-0.927212</td>\n",
       "      <td>-0.544297</td>\n",
       "      <td>-0.901103</td>\n",
       "      <td>-0.148624</td>\n",
       "      <td>-1.386122</td>\n",
       "      <td>-1.033684</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.065639</td>\n",
       "      <td>0.068508</td>\n",
       "      <td>-0.717240</td>\n",
       "      <td>0.186684</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>-0.754385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Alcohol %  Malic Acid       Ash  Alkalinity        Mg   Phenols  \\\n",
       "0     1.518613   -0.562250  0.232053   -1.169593  1.913905  0.808997   \n",
       "1     0.295700    0.227694  1.840403    0.451946  1.281985  0.808997   \n",
       "2     2.259772   -0.625086 -0.718336   -1.650049 -0.192495  0.808997   \n",
       "3     1.382733   -0.768712 -0.170035   -0.809251 -0.332922 -0.152402   \n",
       "4     0.925685   -0.544297  0.158946   -1.049479 -0.754202  0.488531   \n",
       "..         ...         ...       ...         ...       ...       ...   \n",
       "173   0.493343    2.031997  1.803849    1.653086  0.860705 -0.504914   \n",
       "174   0.332758    1.744744 -0.389355    0.151661  1.422412 -1.129824   \n",
       "175   0.209232    0.227694  0.012732    0.151661  1.422412 -1.033684   \n",
       "176   1.395086    1.583165  1.365208    1.502943 -0.262708 -0.392751   \n",
       "177  -0.927212   -0.544297 -0.901103   -0.148624 -1.386122 -1.033684   \n",
       "\n",
       "     Flavanoids  Phenols.1  Proantho-cyanins  Color intensity       Hue  \\\n",
       "0      1.034819  -0.659563          1.224884         0.251717  0.362177   \n",
       "1      0.663351   0.226796          0.401404        -0.319276  0.362177   \n",
       "2      0.954502  -0.578985          0.681738         0.061386  0.537671   \n",
       "3      0.402320  -0.820719         -0.036617        -0.025128  0.932531   \n",
       "4      0.733629  -0.578985          0.383884         0.234414  0.844785   \n",
       "..          ...        ...               ...              ...       ...   \n",
       "173   -1.073511  -0.740141         -0.842575         1.488867 -1.261138   \n",
       "174   -1.344582   0.549108         -0.422075         2.224236 -1.612125   \n",
       "175   -1.354622   1.354888         -0.229346         1.834923 -1.568252   \n",
       "176   -1.274305   1.596623         -0.422075         1.791666 -1.524378   \n",
       "177    0.000733   0.065639          0.068508        -0.717240  0.186684   \n",
       "\n",
       "     OD280 315   Proline  \n",
       "0     1.847920  1.013009  \n",
       "1     0.449601 -0.037874  \n",
       "2     0.336606  0.949319  \n",
       "3     0.294232  1.697675  \n",
       "4     0.407228  1.825055  \n",
       "..         ...       ...  \n",
       "173  -0.976966 -0.372246  \n",
       "174  -1.485445  0.280575  \n",
       "175  -1.400699  0.296498  \n",
       "176  -1.428948 -0.595160  \n",
       "177   0.788587 -0.754385  \n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=data\n",
    "feats=df.drop(['Start assignment','ranking'],axis=1)\n",
    "rankings=df['ranking']\n",
    "avg=np.average(feats,axis=0)\n",
    "std=np.std(feats,axis=0)\n",
    "feats=feats-avg\n",
    "feats=feats/std\n",
    "feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also do it through the sklearn package using [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.51861254, -0.5622498 ,  0.23205254, ...,  0.36217728,\n",
       "         1.84791957,  1.01300893],\n",
       "       [ 0.29570023,  0.22769377,  1.84040254, ...,  0.36217728,\n",
       "         0.44960118, -0.03787401],\n",
       "       [ 2.25977152, -0.62508622, -0.7183361 , ...,  0.53767082,\n",
       "         0.33660575,  0.94931905],\n",
       "       ...,\n",
       "       [ 0.20923168,  0.22769377,  0.01273209, ..., -1.56825176,\n",
       "        -1.40069891,  0.29649784],\n",
       "       [ 1.39508604,  1.58316512,  1.36520822, ..., -1.52437837,\n",
       "        -1.42894777, -0.59516041],\n",
       "       [-0.92721209, -0.54429654, -0.90110314, ...,  0.18668373,\n",
       "         0.78858745, -0.7543851 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x=data.drop([\"Start assignment\",\"ranking\"],axis=1).values\n",
    "y=data['ranking'].values\n",
    "scaler = StandardScaler()\n",
    "x_norm = scaler.fit_transform(x)\n",
    "x_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split and K-fold cross validation\n",
    "Documentation for [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)<br>\n",
    "Split our dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as skl_model\n",
    "train_feat,test_feat,train_ranking,test_ranking=skl_model.train_test_split(x_norm, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 wines for training and 36 for testing\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(train_ranking)} wines for training and {len(test_ranking)} for testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=3, random_state=None, shuffle=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = skl_model.KFold(n_splits = 3, shuffle = True)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  [  1   2   4   5   6   7   9  11  12  15  17  18  19  20  21  22  23  24\n",
      "  25  26  27  28  30  31  32  33  34  35  39  41  42  43  44  46  50  51\n",
      "  52  53  55  56  57  58  61  62  63  64  66  67  68  69  70  71  74  75\n",
      "  76  79  80  81  82  83  84  85  88  90  92  93  94  98  99 100 102 103\n",
      " 105 107 108 111 113 114 115 116 117 119 124 125 127 129 130 133 134 135\n",
      " 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 156 159 162\n",
      " 164 165 166 168 169 171 173 174 175 176] Test:  [  0   3   8  10  13  14  16  29  36  37  38  40  45  47  48  49  54  59\n",
      "  60  65  72  73  77  78  86  87  89  91  95  96  97 101 104 106 109 110\n",
      " 112 118 120 121 122 123 126 128 131 132 136 137 138 154 155 157 158 160\n",
      " 161 163 167 170 172 177]\n",
      "Train:  [  0   1   3   4   5   7   8   9  10  13  14  15  16  18  20  21  22  25\n",
      "  26  27  28  29  33  34  35  36  37  38  39  40  41  42  44  45  46  47\n",
      "  48  49  51  53  54  55  58  59  60  63  65  66  67  69  72  73  74  75\n",
      "  76  77  78  80  82  83  84  86  87  89  91  94  95  96  97  99 100 101\n",
      " 104 106 107 109 110 111 112 115 116 118 119 120 121 122 123 126 127 128\n",
      " 131 132 133 136 137 138 140 141 142 144 145 149 150 153 154 155 156 157\n",
      " 158 159 160 161 163 167 169 170 172 173 177] Test:  [  2   6  11  12  17  19  23  24  30  31  32  43  50  52  56  57  61  62\n",
      "  64  68  70  71  79  81  85  88  90  92  93  98 102 103 105 108 113 114\n",
      " 117 124 125 129 130 134 135 139 143 146 147 148 151 152 162 164 165 166\n",
      " 168 171 174 175 176]\n",
      "Train:  [  0   2   3   6   8  10  11  12  13  14  16  17  19  23  24  29  30  31\n",
      "  32  36  37  38  40  43  45  47  48  49  50  52  54  56  57  59  60  61\n",
      "  62  64  65  68  70  71  72  73  77  78  79  81  85  86  87  88  89  90\n",
      "  91  92  93  95  96  97  98 101 102 103 104 105 106 108 109 110 112 113\n",
      " 114 117 118 120 121 122 123 124 125 126 128 129 130 131 132 134 135 136\n",
      " 137 138 139 143 146 147 148 151 152 154 155 157 158 160 161 162 163 164\n",
      " 165 166 167 168 170 171 172 174 175 176 177] Test:  [  1   4   5   7   9  15  18  20  21  22  25  26  27  28  33  34  35  39\n",
      "  41  42  44  46  51  53  55  58  63  66  67  69  74  75  76  80  82  83\n",
      "  84  94  99 100 107 111 115 116 119 127 133 140 141 142 144 145 149 150\n",
      " 153 156 159 169 173]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(x_norm):\n",
    "    print('Train: ', train_index, \"Test: \", test_index)\n",
    "    x_train, x_test = x_norm[train_index], x_norm[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Framework for doing K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold(k,Xs,ys):\n",
    "    # The total number of examples for training the network\n",
    "    total_num=len(Xs)\n",
    "    # Built in K-fold function in Sci-Kit Learn\n",
    "    kf=skl_model.KFold(n_splits=k,shuffle=True)\n",
    "    \n",
    "    # kf.split: Generate indices to split data into training and test set.\n",
    "    for train_selector,test_selector in kf.split(range(total_num)):\n",
    "        # Decite training examples and testing examples for this fold\n",
    "        train_Xs=Xs[train_selector]\n",
    "        test_Xs=Xs[test_selector]\n",
    "        train_ys=ys[train_selector]\n",
    "        test_ys=ys[test_selector]\n",
    "        \n",
    "        val_array=[]\n",
    "        # Split training examples further into training and validation\n",
    "        train_in,val_in,train_real,val_real=skl_model.train_test_split(train_Xs,train_ys)\n",
    "        \n",
    "        # Fit the data to your model\n",
    "        # Train the model on your data\n",
    "        ...\n",
    "        model = LinearRegression()  # Tutorial\n",
    "        model.init()                # Tutorial\n",
    "        for _ in range(max_epoch):\n",
    "            # Train model on a number of epochs, and test performance in the validation set\n",
    "            ...\n",
    "            error = model.fit(train_in)      # Tutorial\n",
    "            model.update()                   # Tutorial\n",
    "            val_error = model(val_in)        # Tutorial\n",
    "            val_array.append(val_error)      # Tutorial\n",
    "\n",
    "        # Report result for the fold with minimum error in validation set\n",
    "        train_error=model.evaluate(train_Xs,train_ys)\n",
    "        test_error=model.evaluate(test_Xs,test_ys)\n",
    "        print(\"Train error:\",train_error)\n",
    "        print(\"Test error:\",test_error)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L. Prechelt, \"Early Stopping -- but when?\", Neural Networks: Tricks of the trade. Springer, Berlin, Heidelberg, 1998. 55-69.\n",
    "[Link](https://link.springer.com/content/pdf/10.1007%2F978-3-642-35289-8_5.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend for a  minute that in our wine dataset ranks and start assignments are not labels but 2 categorical features, and we want to use one-hot encoders to describe them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the features into categorical features and continuous features, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "categorical_feats=df[['Start assignment','ranking']]\n",
    "continuous_feats=x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [one-hot encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to transform the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can stack categorical and continuous features together for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use one-hot encoders to encode the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encoder=OneHotEncoder()\n",
    "y = np.array(df['ranking']).reshape(-1,1)\n",
    "output_encoder.fit(y)\n",
    "print(output_encoder.transform(y).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can decode the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "\n",
    "Let's try regression of function\n",
    "$$f(x,y)=3x+2y-5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X(number):\n",
    "    xs=(np.random.random(number)*2-1)*10\n",
    "    ys=(np.random.random(number)*2-1)*10\n",
    "    return np.hstack([xs.reshape(-1,1),ys.reshape(-1,1)])\n",
    "    \n",
    "def generate_data(number,stochascity=0.05):\n",
    "    X=generate_X(number)\n",
    "    xs=X[:,0]\n",
    "    ys=X[:,1]\n",
    "    fs=3*xs+2*ys-5\n",
    "    stochastic_ratio=(np.random.random(number)*2-1)*stochascity+1\n",
    "    return X,fs*stochastic_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "x,y=generate_data(5000,0.1)\n",
    "fig=plt.figure()\n",
    "ax=fig.gca(projection='3d')\n",
    "ax.scatter(x[:,0],x[:,1],y,s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X,y=generate_data(1000)\n",
    "reg=...\n",
    "print(reg.score(X,y))\n",
    "print(reg.coef_,reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=generate_X(5000)\n",
    "y=...\n",
    "fig=plt.figure()\n",
    "ax=fig.gca(projection='3d')\n",
    "ax.scatter(X[:,0],X[:,1],y,s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
